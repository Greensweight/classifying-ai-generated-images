{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlazPuz_TC_E"
      },
      "source": [
        "# CPE695 Final Project for Team 1: <br>\n",
        "**Group Members**: Ronald \"Joey\" Rupert, Andrew Greensweight, Michael Salek <br><br>\n",
        "**Problem Statement:** <br>\n",
        "The quality of AI-generated images has rapidly increased, leading to concerns of authenticity and trustworthiness. The aim of this project is to investigate whether computer vision techniques can effectively detect when images have been generated by AI. By addressing this problem, we can contribute to the development of algorithms that enhance the authenticity verification of images.\n",
        "<br>\n",
        "<br>\n",
        "**Information on Dataset:** <br>\n",
        " https://www.kaggle.com/datasets/birdy654/cifake-real-and-ai-generated-synthetic-images\n",
        "The dataset contains two classes - REAL and FAKE. For REAL, the images are collected from Krizhevsky & Hinton's CIFAR-10 dataset. For the FAKE images, they were generated to be the  equivalent of CIFAR-10 with Stable Diffusion version 1.4.There are 100,000 images for training (50k per class) and 20,000 for testing (10k per class).\n",
        "<br>\n",
        "\n",
        "\n",
        "# Content in this .ipynb file:\n",
        "\n",
        "\n",
        "*   Loading in the data\n",
        "*   Pre-Processing the data\n",
        "*   One of three algorithms (K-NN Clustering)\n",
        "*   Results of K-NN Clustering\n",
        "<br>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNBj9x9lTyJx"
      },
      "source": [
        "# Loading in the Data\n",
        "Author: Andrew Greensweight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "z2ezynIsTBcB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from skimage.io import imread\n",
        "from skimage.transform import resize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from google.colab import drive\n",
        "\n",
        "#Currently using 5k real and 5k fake images for the training set because it was too much data to upload to Colab\n",
        "#The test set consists of 1.5k real images and 1.5k fake images\n",
        "real_folder_0 = \"/content/drive/MyDrive/CPE 695 - Summer 2023/Smaller Dataset for Final Project/real_0\"\n",
        "fake_folder_0 = \"/content/drive/MyDrive/CPE 695 - Summer 2023/Smaller Dataset for Final Project/fake_0\"\n",
        "\n",
        "test_real_folder = \"/content/drive/MyDrive/CPE 695 - Summer 2023/Smaller Dataset for Final Project/test_real_0\"\n",
        "test_fake_folder = \"/content/drive/MyDrive/CPE 695 - Summer 2023/Smaller Dataset for Final Project/test_fake_0\"\n",
        "\n",
        "def load_images_from_folder(folder):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for filename in os.listdir(folder):\n",
        "        if filename.endswith(\".jpg\"):\n",
        "            image = cv2.imread(os.path.join(folder, filename))\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
        "            images.append(image)\n",
        "            labels.append(\"real\" if \"real\" in folder else \"fake\")  # Assign labels based on the folder name\n",
        "    return images, labels\n",
        "\n",
        "\n",
        "# Load real images and labels\n",
        "real_images, real_labels = load_images_from_folder(real_folder_0)\n",
        "\n",
        "# Load fake images and labels\n",
        "fake_images, fake_labels = load_images_from_folder(fake_folder_0)\n",
        "\n",
        "# Combine real and fake images and labels\n",
        "training_images = np.concatenate((real_images, fake_images), axis=0)\n",
        "training_labels = np.concatenate((real_labels, fake_labels), axis=0)\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "random_seed = 42\n",
        "\n",
        "# Shuffle the training set\n",
        "np.random.seed(random_seed)\n",
        "shuffle_indices = np.random.permutation(len(training_images))\n",
        "training_images = training_images[shuffle_indices]\n",
        "training_labels = training_labels[shuffle_indices]\n",
        "\n",
        "# Combine the training set\n",
        "combined_data = list(zip(training_images, training_labels))\n",
        "#Tuples of images and labels\n",
        "training_images, training_labels = zip(*combined_data)\n",
        "\n",
        "#Convert the images and labels back into NumPy arrays for further processing\n",
        "training_images = np.array(training_images)\n",
        "training_labels = np.array(training_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-Processing\n",
        "Perform geometry augmentation shifts and use a Pre-trained CNN to extract features\n",
        "<br>\n",
        "Author: Andrew Greensweight<br>"
      ],
      "metadata": {
        "id": "QQ0kAGcuIKWn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "At1boh8jI6sR",
        "outputId": "dd1b8ead-c73a-4f6c-b7cb-c6367d1d5c8e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.56.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.10)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.0)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.22.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.6.3)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (0.2.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage.transform import resize\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import tensorflow.keras.applications as keras_applications\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Define image dimensions\n",
        "image_size = (32, 32)\n",
        "\n",
        "# Define the number of neighbors for KNN\n",
        "n_neighbors = 5\n",
        "\n",
        "# Define the geometry augmentation parameters\n",
        "augmentation_params = {\n",
        "    \"flip\": True,\n",
        "    \"shift_range\": 0.2\n",
        "}\n",
        "\n",
        "def apply_geometry_augmentations(images, labels, augmentation_params):\n",
        "    datagen = ImageDataGenerator(\n",
        "        horizontal_flip=augmentation_params[\"flip\"],\n",
        "        width_shift_range=augmentation_params[\"shift_range\"],\n",
        "        height_shift_range=augmentation_params[\"shift_range\"]\n",
        "    )\n",
        "    augmented_images = []\n",
        "    augmented_labels = []\n",
        "    for image, label in zip(images, labels):\n",
        "        augmented_images.append(image)\n",
        "        augmented_labels.append(label)\n",
        "        if augmentation_params[\"flip\"]:\n",
        "            flipped_image = cv2.flip(image, 1)\n",
        "            augmented_images.append(flipped_image)\n",
        "            augmented_labels.append(label)\n",
        "        if augmentation_params[\"shift_range\"]:\n",
        "            shifted_image = datagen.random_transform(image)\n",
        "            augmented_images.append(shifted_image)\n",
        "            augmented_labels.append(label)\n",
        "    return augmented_images, augmented_labels\n",
        "\n",
        "\n",
        "\n",
        "# Load real images and labels for the test set\n",
        "test_real_images, test_real_labels = load_images_from_folder(test_real_folder)\n",
        "test_fake_images, test_fake_labels = load_images_from_folder(test_fake_folder)\n",
        "\n",
        "# Combine real and fake images and labels for the test set\n",
        "test_images = np.concatenate((test_real_images, test_fake_images), axis=0)\n",
        "test_labels = np.concatenate((test_real_labels, test_fake_labels), axis=0)\n",
        "\n",
        "# Load pre-trained MobileNetV2 model (without the top classifier)\n",
        "model = keras_applications.MobileNetV2(include_top=False, weights='imagenet', input_shape=(image_size[0], image_size[1], 3))\n",
        "\n",
        "# Reshape the images to match the input shape of MobileNetV2\n",
        "reshaped_training_images = []\n",
        "for image in training_images:\n",
        "    resized_image = resize(image, image_size)\n",
        "    reshaped_training_images.append(resized_image)\n",
        "reshaped_training_images = np.array(reshaped_training_images)\n",
        "\n",
        "reshaped_test_images = []\n",
        "for image in test_images:\n",
        "    resized_image = resize(image, image_size)\n",
        "    reshaped_test_images.append(resized_image)\n",
        "reshaped_test_images = np.array(reshaped_test_images)\n",
        "\n",
        "# Flatten the images to use as input features for the KNN classifier\n",
        "X_train = reshaped_training_images.reshape(len(reshaped_training_images), -1)\n",
        "y_train = training_labels\n",
        "X_test = reshaped_test_images.reshape(len(reshaped_test_images), -1)\n",
        "y_test = test_labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "-QC2A_rhII-N",
        "outputId": "80d12811-0302-40d8-fb89-8211807f0699"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "9406464/9406464 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"X_train:\")\n",
        "print(X_train[:10])\n",
        "print(\"X_test:\")\n",
        "print(X_test[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "nKW1U4CtSsSn",
        "outputId": "0407dc6b-6768-42a4-9af2-3940742ca237"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train:\n",
            "[[0.11372549 0.08235294 0.         ... 0.23921569 0.21960784 0.        ]\n",
            " [1.         0.59215686 0.01568627 ... 0.21568627 0.1254902  0.09411765]\n",
            " [0.70588235 0.74509804 0.78039216 ... 0.38039216 0.44705882 0.50980392]\n",
            " ...\n",
            " [0.79215686 0.79215686 0.8        ... 0.48627451 0.48627451 0.49411765]\n",
            " [0.03137255 0.05490196 0.01568627 ... 0.37254902 0.27058824 0.14117647]\n",
            " [0.43137255 0.54901961 0.69803922 ... 0.29803922 0.39607843 0.42352941]]\n",
            "X_test:\n",
            "[[0.36470588 0.2627451  0.17254902 ... 0.52941176 0.41568627 0.28235294]\n",
            " [0.82352941 0.81176471 0.75294118 ... 0.43529412 0.54509804 0.38431373]\n",
            " [0.81960784 0.76470588 0.72941176 ... 0.58823529 0.65098039 0.54901961]\n",
            " ...\n",
            " [0.20392157 0.33333333 0.29803922 ... 0.20784314 0.25882353 0.15686275]\n",
            " [0.98823529 0.89803922 0.86666667 ... 0.97647059 0.85098039 0.70196078]\n",
            " [0.94901961 0.94117647 0.98431373 ... 0.08235294 0.09019608 0.07058824]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create and fit the KNN classifier\n",
        "Author: Andrew Greensweight"
      ],
      "metadata": {
        "id": "sgHa7LTBRqWu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Define the parameter grid for grid search\n",
        "param_grid = {\n",
        "    'n_neighbors': [1, 3, 5, 7, 9, 11, 13, 15, 31],\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'p': [1, 2]  # For Minkowski distance: p=1 for Manhattan distance, p=2 for Euclidean distance\n",
        "}\n",
        "\n",
        "# Create the KNN classifier\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "# Perform grid search\n",
        "grid_search = GridSearchCV(estimator=knn, param_grid=param_grid, scoring='accuracy', cv=3)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and accuracy\n",
        "best_params = grid_search.best_params_\n",
        "best_accuracy = grid_search.best_score_\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Best Accuracy:\", best_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "sT0EAYmgRJ1D",
        "outputId": "c5b3eaa6-be0c-48ae-a43f-c5e7f9992ca5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'n_neighbors': 31, 'p': 1, 'weights': 'uniform'}\n",
            "Best Accuracy: 0.6860003336866379\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parameter Tuning and Results\n",
        "Now for using the best parameters determined by grid search on the test set.<br>\n",
        "Author: Andrew Greensweight<br>"
      ],
      "metadata": {
        "id": "KMdNTHWAEiRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the \"best\" KNN classifier\n",
        "knn_best = KNeighborsClassifier(**best_params)\n",
        "\n",
        "#Fit the new classifier\n",
        "knn_best.fit(X_train, y_train)\n",
        "\n",
        "#Predict the labels for the test set\n",
        "y_pred = knn_best.predict(X_test)\n",
        "\n",
        "#Calculate the accuracy on the test set\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "_N6ikEIMEote",
        "outputId": "eeb7db9b-2ac7-405e-cadb-5e36c829c660"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.6903333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Best accuracy using the test set is measured to be 69.03%"
      ],
      "metadata": {
        "id": "TCPL13sR0xcj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Confusion Matrix\n",
        "Author: Andrew Greensweight"
      ],
      "metadata": {
        "id": "TqxF6uxhGWXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Create a confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Define class labels\n",
        "class_labels = np.unique(y_test)\n",
        "\n",
        "# Create a DataFrame for the confusion matrix\n",
        "df_cm = pd.DataFrame(cm, index=class_labels, columns=class_labels)\n",
        "\n",
        "# Display the confusion matrix as a table\n",
        "print(\"Confusion Matrix:\")\n",
        "print(df_cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "vSTW0tkeGg_m",
        "outputId": "4c0638fc-90ec-436c-d32c-f5d588307fed"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "      fake  real\n",
            "fake   913   587\n",
            "real   342  1158\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}